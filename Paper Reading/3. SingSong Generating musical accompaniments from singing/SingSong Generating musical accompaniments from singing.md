# SingSong：从 SingSong 中产生音乐伴奏

**Chris Donahue，Antoine Caillon，Adam Roberts，Ethan Manilow，Philippe Esling，Andrea Agostinelli，Mauro Verzetti，Ian Simon，Olivier Pietquin，Neil Zeghidour，Jesse Engel**

> **观后简言：**
>
> 



#### 摘要

我们介绍了 SingSong，这是一个通过人声进行音乐生成的系统，有可能为音乐家和非音乐家提供一种直观的新方式来创作具有自己声音特色的音乐。为了实现这一点，我们在音乐源分离和音频生成方面的最新发展的基础上进行了研究。具体来说，我们将最先进的源分离算法应用于大型音乐音频语料库，以产生对齐的人声和乐器源。然后，我们将 AudioLM（Borsos等人，2022）——一种最先进的无条件音频生成方法——调整为适用于有条件的“音频到音频”生成任务，并在源分离（声乐、器乐）对上对其进行训练。在与相同声音输入的成对比较中，与来自强检索基线的相比，听众表示 SingSong 生成的音乐伴奏有显著的提高。



#### 1. 简介

唱歌是参与音乐的最直观的途径之一，无论是音乐家还是非音乐家都可以使用。尽管跟着现有音乐唱歌是一种常见的活动，但唱歌也可能构成音乐生成系统的直观控制机制，可能会让任何会唱歌的人以一种有趣和参与的方式创作音乐。

在这项工作中，我们提出了 SingSong（图 1），这是一个能够生成器乐音频来同步伴随输入人声音频的系统，即输出的乐器可以与输入人声天然混合，以创建具有输入特征的连贯音乐。SingSong 利用音乐技术的两个关键领域的改进：源分离和音频的生成建模。我们使用 Kim 等人的现成源分离算法。（2021）将大量不同的音乐语料库（1M 首曲目）分离为对齐的人声和乐器源对，构成我们任务的并行数据。然后，我们改编了 AudioLM（Bor-sos 等人，2022）——一个涉及中间表示层次的音频无条件生成模型——适用于给定人声的乐器的条件“音频到音频”生成建模，并以监督的方式在源分离数据上对其进行训练。

![图 1 SingSong 生成器乐来伴随输入人声，从而允许用户创建以自己的声音为特色的音乐。（左）我们通过将现成的源分离算法应用于大型音乐音频语料库来为这项任务制造大量的合成数据，我们使用该算法来训练给定人声的乐器的生成模型。（右）在推断的时候，SingSong 从用户那里获取人声，并输出一个乐器来伴奏这些人声，这些乐器可以天然的与输入混合，创造出连贯的音乐
*请注意，在训练过程中，我们计算离散音频特征的损失，而不是波形（第 3.3 节）](img/1.png)

这项工作中的一个关键挑战是建立一个系统，该系统能够从训练期间观察到的源分离的人声输入推广到现实世界中的孤立人声，人们可以从参与该系统的用户那里预期。初步实验得出的模型强烈倾向于从源分离人声中几乎听不见的伪影（conceal artifacts）中重建乐器——当输入孤立人声时，这些模型产生了荒谬的输出。为了提高泛化能力，我们为输入人声提出了两种特征化策略：（1）向人声输入添加噪声以隐藏伪影，以及（2）仅使用 AudioLM 中最粗糙的中间表示作为条件输入。在感知相关的指标上，与默认的 AudioLM 特征化相比，这些特征化共同将孤立视频的性能提高了 55%。

在一项配对研究中，听众被呈现出两种人声相同的声乐器乐混音，与强检索基线相比，听众对 SingSong 的器乐表现出显著的偏好。该基线使用 vocals 的音乐特征（节拍和键）作为查询，以检索具有相似特征的人类创作的器乐。与这个检索基线的乐曲相比，66%的时间听众更喜欢 SingSong 的乐曲。此外，即使与现实乐器相比，听众也有 34% 的时间更喜欢 SingSong 的乐曲。

总之，我们的主要贡献是：

- 我们是第一个使用生成建模为人声输入创建连贯乐器伴奏的公司。
- 我们是第一个提出使用源分离来创建音频到音频伴奏的训练数据的公司。
- 我们采用最先进的无条件音频生成模型进行有条件的音频到音频建模。



#### 2. 相关工作

**伴奏生成。**与我们自己的工作最相似的是一个名为 Microsoft Songsmith 的商业产品，基作者是 Simon等人（2008）。Songsmith 从输入人声中提取音高信息，并预测适合该旋律的符号和弦标签序列。然后，预测的和弦被用作查询，以检索与输入人声混合的合适的人类创作的象征性乐器伴奏。我们直接与我们构建的类似检索基线进行比较。Lattner 和 Grachten（2019）生成象征性的低音鼓伴奏，Grachten 等人（2020）生成低音伴奏。Wu 等人（2022）在没有鼓的情况下，在给定音乐音频的情况下生成鼓曲目的音频。在这里，我们考虑的是仅基于声乐表演生成整个伴奏曲目（可能包含鼓、低音和其他和声元素）的更广泛任务。还有一系列关于符号协调的工作，即预测适合符号旋律输入的和弦标签（Paiement 等人，2006；Yeh 等人，2020）。为了适合声乐伴奏，这种方法需要额外的声乐转录和和弦编排步骤。

**音乐音频生成。**音乐音频生成是一项具有挑战性的任务，因为它涉及许多不同音频尺度的建模，从局部信息（例如，音色、瞬态（transients））到全局信息（例如音乐结构、流派）。一些关于无条件音乐生成的工作直接对时域音乐波形进行操作（van den Oord 等人，2016；Mehri 等人，2017；Donahue 等人，2019；Goel 等人，2022）。这些方法仅限于对音乐音频（通常是钢琴音乐）的狭窄分布进行建模。van den Oord 等人（2017）介绍了一种对音乐音频的可逆离散表示进行生成建模的方法。Dieleman 等人（2018）对钢琴音乐音频的分层离散特征进行了建模——Dhariwal 等人也使用了类似的方法并在 2020 年创建了 Jukebox，这是第一个广义音乐音频的通用模型。霍桑等人（2022）；
Borsos 等人（2022）还对钢琴音乐的离散音乐特征进行了建模。大量的工作围绕着以符号表示为条件的可控音乐音频生成展开（Hawthorne 等人，2018；Engel 等人，2019；Hawthonne 等人，2022）或表演特征（Caillon 和 Esling，2021；Engel 等人，2020）。在我们的工作中，我们试图对以声乐音频为条件的器乐音频的广泛分布进行建模。

与这项工作同期，Agostinelli 等人（2023）提出了 MusicLM，该模型适用于 AudioLM，以生成基于输入文本描述的广泛音乐音频。MusicLM 还能够生成具有与用户的旋律提示相对应的旋律的音乐，通过唱歌或哼唱来表达。在这里，我们的目标是与输入人声同步生成教学音乐，这样它们就可以连贯地混合在一起——人声不仅像 MusicLM 中那样用于指导音乐生成，而且还直接出现在输出音乐中。



#### 3. 任务定义和方法

我们将声乐伴奏的任务视为一个条件生成建模问题，其中目标是对声乐波形 x 在适当的乐器波形 y 上的分布 P(y|x) 进行建模，其中两个波形都是单声道的，长度为 T 秒，并以一定的速率 f~s~ 采样。形式上，x 和 y 都是
$$
R^{f_sT}
$$
中的向量。鉴于人声 x‘，我们可以通过采样 y’～P(y|x=x‘)并线性混合输出（例如 x’+y‘）来生成包含这些人声的音乐。

##### 3.1 对音频代码的代理分布进行建模

由于表示音频所需的高采样率，直接对波形上的分布建模在实际中具有挑战性。采用 van den Oord 等人首次提出的做法。（2017）对音频进行无条件通用建模，在这项工作中，我们转而对离散音频代码进行建模。这种方法需要一个离散的编解码器，一对函数 
$$
Enc:R^{f_sT} → V^{f_kT} 和 Dec:V^{f_kT} →  R^{f_sT}
$$
，其中 V 是一些有限词汇，f~k~ 是音频码率，Dec 近似于 Enc^−1^。

使用这两个函数，我们可以为“代理（proxy）”建模 Enc 产生的代码上的分布，并通过利用 Dec 来近似波形上的分布。具体地说，对于目标是对波形 w 上的分布进行建模的无条件设置，我们可以将 P(ŵ) 建模为代理，其中 ŵ = Enc(w)。为了从该分布中对音频进行采样，我们首先对 ŵ’～P(ŵ) 进行采样，并输出 Dec(ŵ‘)。由于在实践中 f~k~ << f~s~，对该代理进行建模在理论上是容易处理的。

##### 3.2 初步AudioLM

在这项工作中，我们的方法基于 AudioLM（Borsos 等人，2022），这是一种最先进的无条件音频生成模型，它使用因子分解方法对多种类型的音频代码上的代理分布进行建模。

**声学代码 AudioLM** 中的离散编解码器是一个预训练的 SoundStream（Zeghidour 等人，2021）模型，
$$
带有编码器 Enc:R^{f_sT} → V^{50T × 12}和相应的解码器 Dec，其中f_s=16kHz以及|V| = 1024。
$$
Enc 的输出称为声学代码。Sound Stream 使用残差矢量量化方案，以 50Hz 的速率产生 12 维声学代码矢量——前 4 维和其余 8 维分别是粗略和精细声学代码。AudioLM “压缩”这些粗码和细码，每秒音频的诱导率分别为 200 和 400。我们注意到波形 w 的平坦粗声码为 coarse(w)，平坦细声码为 fine(w)。请注意，由于其残差量化方案，SoundStream 可以单独从粗码解码音频，尽管从粗码和细码解码会产生更高的音频保真度。

**语义代码** 过去的音乐音频生成模型，如 Jukebox（Wu 等人，2022），已经用更大的模型直接建模了类似于声学代码的再现。相反，AudioLM 建议使用较小的模型来对
$$
源自语义编码器 Sem:R^{f_sT} → S^{25T}在 25 Hz 下工作，并用自监督损失而不是声学重建损失进行训练。
$$
语义编码器由来自预先训练的 w2v-BERT 模型（Chung 等人，2021）的中间表示组成，这些中间表示已经用 k- 均值量化，其中 k = |S| = 1024。

AudioLM 是三个模型的级联，它们生成越来越高的速率代码。具体而言，AudioLM 将语义和声学代码的联合分布分解为：
$$
P(Sem(w), Enc(w)) = \\
P(Fine(w) | Coarse(w)) \\
· P(Coarse(w) | Sem(w)) \\
· P(Sem(w))
$$
假设 Fine(w) ⫫ Sem(w)|Coarse(w)，即语义代码对于生成给定粗糙代码的精细声学代码是无益的。每个模型都是一个自动的、仅限解码器的转换器（Vaswani 等人，2017）“语言模型”是单独训练的，条件反射是通过简单地连接两个序列来实现的。

为了用 AudioLM 生成音频，首先对三个生成模块进行串行采样。然后，生成的语义代码作为副产品被丢弃，并且生成的声学代码被馈送到 Dec，从而产生波形。

##### 3.3 将 AudioLM 改编为伴奏